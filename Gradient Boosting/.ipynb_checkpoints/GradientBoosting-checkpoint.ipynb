{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = pickle.load(open(\"../../Data.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70655"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cortamos por precios\n",
    "prop = prop[(prop['price_usd_per_m2']>150) & (prop['price_usd_per_m2']<8000)]\n",
    "len(prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigoderosa/.local/lib/python2.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "p = prop[['property_type','place_name','surface_total_in_m2','surface_covered_in_m2',\\\n",
    "          'price_usd_per_m2', 'price_aprox_usd', 'latlon','Cant_ColeYUniv','Cant_LocalesGastronomicos',\n",
    "         'Cant_LugaresTuristicos']]\n",
    "p.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66943"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigoderosa/.local/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "p['lat'] = p.apply(lambda row: row[6][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigoderosa/.local/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "p['lon'] = p.apply(lambda row: row[6][1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p[['property_type','place_name','surface_total_in_m2','surface_covered_in_m2',\\\n",
    "          'price_usd_per_m2', 'price_aprox_usd', 'lat','lon','Cant_ColeYUniv','Cant_LocalesGastronomicos',\n",
    "         'Cant_LugaresTuristicos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se le asigna un valor numerico al tipo de propiedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignarPT( p ):\n",
    "    if p == 'apartment':\n",
    "        return 3\n",
    "    if p == 'house':\n",
    "        return 2.25\n",
    "    if p == 'store':\n",
    "        return 6.5\n",
    "    return 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['property_type'] = p.apply(lambda x: asignarPT(x[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se le asigna un valor a los barrios segun el analisis de grupos hecho en el tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignarPV(name, dic):\n",
    "    return dic[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoodGroups = p[[\"place_name\", \"price_usd_per_m2\"]]\n",
    "hoodGroups = hoodGroups.groupby(\"place_name\").agg([np.mean]).reset_index()\n",
    "hoodGroups = hoodGroups.sort_values(by=(\"price_usd_per_m2\", \"mean\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGdics = {}\n",
    "precio_ant = 0\n",
    "val_ant = 0\n",
    "for row in hoodGroups.iterrows():\n",
    "    name = row[1][0]\n",
    "    price = row[1][1]\n",
    "    val = val_ant + (precio_ant/price)\n",
    "    HGdics[name] = val\n",
    "    precio_ant = price\n",
    "    val_ant = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"place_value\"] = p.apply(lambda x: asignarPV(x[1], HGdics), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p[['property_type','surface_total_in_m2','surface_covered_in_m2',\\\n",
    "          'price_usd_per_m2', 'price_aprox_usd', 'lat','lon','Cant_ColeYUniv','Cant_LocalesGastronomicos',\n",
    "         'Cant_LugaresTuristicos', 'place_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split del dataset para entrenamiento y pruebas, 80% y 20% respectivamente\n",
    "Xtrn, Xtest, Ytrn, Ytest = train_test_split(p[['property_type','surface_total_in_m2','surface_covered_in_m2',\n",
    "        'lat','lon','Cant_ColeYUniv','Cant_LocalesGastronomicos','Cant_LugaresTuristicos',\n",
    "        'place_value']],p[['price_aprox_usd']],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "loss = ['ls', 'lad', 'huber', 'quantile']\n",
    "learning_rate = [x/100.0 for x in xrange(5, 20, 5)]\n",
    "n_estimators = [x for x in xrange(300, 550, 50)]\n",
    "max_depth = [x for x in xrange(3, 6)]\n",
    "alpha = [x/100.0 for x in xrange(85, 100, 5)]\n",
    "\n",
    "for _loss in loss:\n",
    "    for _learning_rate in learning_rate:\n",
    "        for _n_estimators in n_estimators:\n",
    "            for _max_depth in max_depth:\n",
    "                    if (_loss == 'huber' or _loss == 'quantile'):\n",
    "                        for _alpha in alpha:\n",
    "                            models.append(GradientBoostingRegressor(loss=_loss, n_estimators=_n_estimators, \\\n",
    "                                                                    max_depth=_max_depth, learning_rate=_learning_rate, \\\n",
    "                                                                    alpha = _alpha))\n",
    "                    else:\n",
    "                        models.append(GradientBoostingRegressor(loss=_loss, n_estimators=_n_estimators, \\\n",
    "                                                                max_depth=_max_depth, learning_rate=_learning_rate))\n",
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END: 6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Diccionario para crear un dataframe\n",
    "results = {'Loss' : [], 'Estimators' : [], 'Max Depth' : [], 'Learning Rate' : [], 'Alpha' : [], 'RMSE' : []}\n",
    "\n",
    "for model in models:\n",
    "    #Entrenamos a cada modelo y nos guardamos el error y cuanto tarda en predecir\n",
    "    init = time.time()\n",
    "    model.fit(Xtrn, Ytrn)\n",
    "    end = time.time()\n",
    "    #Error\n",
    "    error = mean_squared_error(Ytest, model.predict(Xtest))\n",
    "    #Guardamos el resultado en el diccionario para luego crear el dataframe\n",
    "    results['Loss'].append(model.loss)\n",
    "    results['Estimators'].append(model.n_estimators)\n",
    "    results['Max Depth'].append(model.max_depth)\n",
    "    results['Learning Rate'].append(model.learning_rate)\n",
    "    if model.loss == 'huber' or model.loss == 'quantile': \n",
    "        results['Alpha'].append(model.alpha)\n",
    "    else:\n",
    "        results['Alpha'].append(-1)\n",
    "    results['RMSE'].append(error)\n",
    "\n",
    "print \"END: \" + str(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un DataFrame donde guardamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( resDF, open( \"gridGB.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los resultados del GridSearch para seguir analizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pickle.load(open(\"gridGB.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Estimators</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Max Depth</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>500</td>\n",
       "      <td>0.15</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>293783.101806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>450</td>\n",
       "      <td>0.15</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>295862.121468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>400</td>\n",
       "      <td>0.15</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>299099.077625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.95</td>\n",
       "      <td>500</td>\n",
       "      <td>0.15</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>300008.621472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>500</td>\n",
       "      <td>0.10</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>301044.649358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alpha  Estimators  Learning Rate   Loss  Max Depth           RMSE\n",
       "44   -1.00         500           0.15     ls          5  293783.101806\n",
       "41   -1.00         450           0.15     ls          5  295862.121468\n",
       "38   -1.00         400           0.15     ls          5  299099.077625\n",
       "224   0.95         500           0.15  huber          5  300008.621472\n",
       "29   -1.00         500           0.10     ls          5  301044.649358"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.sort_values('RMSE').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 5, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 450, max_depth = 5, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 400, max_depth = 5, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'huber', n_estimators = 500, max_depth = 5, learning_rate = 0.15, alpha = 0.95),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 5, learning_rate = 0.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23878322157.187836, 2), (18684453537.402599, 1), (18684453537.402599, 1), (18684453537.402599, 1), (18684453537.402599, 1)]\n"
     ]
    }
   ],
   "source": [
    "prevError = 0\n",
    "index = -1\n",
    "results = []\n",
    "for i in xrange(5):\n",
    "    #split del dataset para entrenamiento y pruebas, 80% y 20% respectivamente\n",
    "    Xtrn, Xtest, Ytrn, Ytest = train_test_split(p[['property_type','surface_total_in_m2','surface_covered_in_m2',\n",
    "        'lat','lon','Cant_ColeYUniv','Cant_LocalesGastronomicos','Cant_LugaresTuristicos',\n",
    "        'place_value']],p[['price_aprox_usd']], test_size=0.2)\n",
    "        #Entrenamos a cada modelo y nos guardamos el error y cuanto tarda en predecir\n",
    "    k = 0\n",
    "    for model in models:\n",
    "        model.fit(Xtrn, Ytrn)\n",
    "        #Error\n",
    "        error = mean_squared_error(Ytest, model.predict(Xtest))\n",
    "        if error < prevError or prevError == 0:\n",
    "            prevError = error\n",
    "            index = k\n",
    "        k += 1\n",
    "    results.append((prevError, index))\n",
    "\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que el primer modelo es el mejor en todos los casos que probamos, por lo que continuamos usando ese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 5, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 5, learning_rate = 0.25),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 5, learning_rate = 0.35),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 3, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 3, learning_rate = 0.25),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 3, learning_rate = 0.35),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 7, learning_rate = 0.15),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 7, learning_rate = 0.25),\n",
    "    GradientBoostingRegressor(loss = 'ls', n_estimators = 500, max_depth = 7, learning_rate = 0.35)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevError = 0\n",
    "index = -1\n",
    "results = []\n",
    "for i in xrange(1):\n",
    "    #split del dataset para entrenamiento y pruebas, 80% y 20% respectivamente\n",
    "    Xtrn, Xtest, Ytrn, Ytest = train_test_split(p[['property_type','surface_total_in_m2','surface_covered_in_m2',\n",
    "        'lat','lon','Cant_ColeYUniv','Cant_LocalesGastronomicos','Cant_LugaresTuristicos',\n",
    "        'place_value']],p[['price_aprox_usd']], test_size=0.1)\n",
    "        #Entrenamos a cada modelo y nos guardamos el error y cuanto tarda en predecir\n",
    "    k = 0\n",
    "    for model in models:\n",
    "        model.fit(Xtrn, Ytrn)\n",
    "        #Error\n",
    "        error = mean_squared_error(Ytest, model.predict(Xtest))\n",
    "        if error < prevError or prevError == 0:\n",
    "            prevError = error\n",
    "            index = k\n",
    "        print \"Index: \" + str(k) + \" | RMSE: \" + str(error)\n",
    "        k += 1\n",
    "    results.append((prevError, index))\n",
    "\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
